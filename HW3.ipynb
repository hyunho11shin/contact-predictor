{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0579b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python libraries (pytorch, numpy, etc -- whatever you need to import)\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import scipy\n",
    "import string\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0231a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "torch.set_num_threads(4) # make it use upto 4 cpus -- to avoid utilizing all CPUs on master node when you run this script on master\n",
    "\n",
    "BATCH_SIZE = 16 # number of batches\n",
    "CROP_SIZE = 64 # If input protein is longer than CROP_SIZE, it will be trimed to have CROP_SIZE residues (i_start:i_start+CROP_SIZE)\n",
    "LR = 0.001 # learning rate (Q. what would happen if you have too high/too small learning rates?)\n",
    "NUM_EPOCHS = 3\n",
    "AA_ORDER = \"ARNDCQEGHILKMFPSTWYVX-\"\n",
    "device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\" # which device to use?\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a560eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, list_name):\n",
    "        self.root = \"/public_data/ml/CATH40/CATH40-20JUN08/\" # home directory for dataset\n",
    "        self.domIDs = [line.strip() for line in open(\"%s/%s\"%(self.root, list_name))] # items in dataset\n",
    "        \n",
    "        self.msa_dir = \"%s/msa\"%self.root # where can I find msa?\n",
    "        self.ccm_dir = \"%s/ccm\"%self.root # where can I find raw CCM data (coevolution analysis)?\n",
    "        self.pdb_dir = \"%s/pdb\"%self.root # where can I find ground-truth structure?\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.domIDs)\n",
    "    \n",
    "    def read_MSA(self, msa_fn):\n",
    "        table = str.maketrans(dict.fromkeys(string.ascii_lowercase))\n",
    "        \n",
    "        msa = list()\n",
    "        with open(msa_fn) as fp:\n",
    "            for line in fp:\n",
    "                # skip labels\n",
    "                if line[0] != '>':\n",
    "                    # remove whitespaces/lowercase letters and append to MSA\n",
    "                    msa.append(line.strip().translate(table))\n",
    "            \n",
    "        # convert letters into numbers\n",
    "        alphabet = np.array(list(AA_ORDER), dtype='|S1').view(np.uint8) # (22)\n",
    "        msa = np.array([list(s) for s in msa], dtype='|S1').view(np.uint8) # (Nseq, L)\n",
    "        for i in range(alphabet.shape[0]):\n",
    "            msa[msa == alphabet[i]] = i\n",
    "\n",
    "        # treat all unknown characters as UNKNOWN\n",
    "        msa[msa >= 22] = 20\n",
    "            \n",
    "        msa_onehot = np.eye(22)[msa] # one-hot encoded msa (Nseq, Length, 22)\n",
    "        seq_onehot = msa_onehot[0] # one-hot encoded query sequence (Length, 22)\n",
    "        seq_profile = msa_onehot.sum(axis=0) / msa_onehot.shape[0] # MSA sequence profile (Length, 22)\n",
    "\n",
    "        return seq_onehot, seq_profile\n",
    "        \n",
    "    def read_pdb(self, pdb_fn, L):\n",
    "        cords = list()\n",
    "        seqnums = list()\n",
    "        with open(pdb_fn) as fp:\n",
    "            for line in fp:\n",
    "                if line.startswith(\"ATOM\"):\n",
    "                    resName = line[17:20].strip()\n",
    "                    atmName = line[12:16].strip()\n",
    "                    resSeqN = line[23:26].strip()\n",
    "                    if resName == \"GLY\":\n",
    "                        if atmName == \"CA\":\n",
    "                            cords.append([float(line[30:38]), float(line[38:46]), float(line[46:54])])\n",
    "                            seqnums.append(int(resSeqN) - 1)\n",
    "                    else:\n",
    "                        if atmName == \"CB\":\n",
    "                            cords.append([float(line[30:38]), float(line[38:46]), float(line[46:54])])\n",
    "                            seqnums.append(int(resSeqN) - 1)\n",
    "        cords = np.array(cords)\n",
    "\n",
    "        Cb_dist = scipy.spatial.distance.cdist(cords, cords)\n",
    "        Cb_contact = (Cb_dist < 8.0).astype(float) # Cb-Cb contact map (Ca for Gly) (Length, Length)\n",
    "        \n",
    "        # In PDB, it can have a missing region (exists in sequence, but not in structure).\n",
    "        # I will use this during loss calculation.\n",
    "        missing_res = list()\n",
    "\n",
    "        for i in range(L):\n",
    "            if i not in seqnums:\n",
    "                Cb_contact = np.insert(Cb_contact, i, np.zeros((1, Cb_contact.shape[1])), axis=0)\n",
    "                Cb_contact = np.insert(Cb_contact, i, np.zeros(1), axis=1)\n",
    "                missing_res.append(i)\n",
    "\n",
    "        mask = np.ones((L, L))\n",
    "        mask[missing_res, :] = 0\n",
    "        mask[:, missing_res] = 0\n",
    "        \n",
    "        return torch.from_numpy(Cb_contact), torch.from_numpy(mask)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        domain = self.domIDs[idx]\n",
    "        msa_fn = \"%s/%s.a3m\"%(self.msa_dir, domain) # msa file for selected domain\n",
    "        ccm_fn = \"%s/%s.npy\"%(self.ccm_dir, domain) # raw ccm file for selected domain\n",
    "        pdb_fn = \"%s/%s.pdb\"%(self.pdb_dir, domain) # raw pdb file for selected domain\n",
    "         \n",
    "        # 1. read MSA & get query sequence + sequence profile & concatenate them to get a 1D feature\n",
    "        feat_1d = np.concatenate(self.read_MSA(msa_fn), axis=-1) # (Length, 22+22)\n",
    "        L = feat_1d.shape[0]\n",
    "        \n",
    "        # 2. tile them to make it as 2D feature\n",
    "        tile_x = np.tile(feat_1d, (L, 1, 1))\n",
    "        tile_y = np.transpose(tile_x, (1, 0, 2))\n",
    "\n",
    "        # 3. read raw CCM (coevolution) data & get 2D input features by concatenating CCM features & tiled 1D features\n",
    "        ccm = np.load(ccm_fn)\n",
    "        feat_2d = torch.from_numpy(np.concatenate((ccm, tile_x, tile_y), axis=-1)) # (L, L, 441+44+44)\n",
    "        \n",
    "        # 4. read pdb & get contacts between Cb atoms (for Gly use Ca instead of Cb)\n",
    "        labels, mask = self.read_pdb(pdb_fn, L)\n",
    "        \n",
    "        # 5. return input features, labels (contact map), and mask information (to calculate loss on valid pairs only)\n",
    "        return feat_2d, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f12325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to define a rule about how to collate batch when proteins have different length\n",
    "# Below is the example. feel free to modify it or use it as-is\n",
    "def collate_batch(batch):\n",
    "    # Input: batch = (input_feat, label, mask)\n",
    "    # Output: Cropped input_feat, label, mask\n",
    "    L_min = min([CROP_SIZE, min([input_feat.shape[0] for (input_feat,_,_) in batch])])\n",
    "    \n",
    "    b_input = list()\n",
    "    b_label = list()\n",
    "    b_mask = list()\n",
    "    # crop examples having length > L_min\n",
    "    for input_feat, label, mask in batch:\n",
    "        L = input_feat.shape[0]\n",
    "        if L > L_min:\n",
    "            end_idx_1 = np.random.randint(L_min, L)\n",
    "            end_idx_2 = np.random.randint(L_min, L)\n",
    "        else:\n",
    "            end_idx_1 = L_min\n",
    "            end_idx_2 = L_min\n",
    "        b_input.append(input_feat[end_idx_1-L_min:end_idx_1,end_idx_2-L_min:end_idx_2])\n",
    "        b_label.append(label[end_idx_1-L_min:end_idx_1,end_idx_2-L_min:end_idx_2])\n",
    "        b_mask.append(mask[end_idx_1-L_min:end_idx_1,end_idx_2-L_min:end_idx_2])\n",
    "    return torch.stack(b_input), torch.stack(b_label), torch.stack(b_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d49b6c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataloader for training set, validation set\n",
    "traindata = CustomDataset(\"train_s\") # (10251,)\n",
    "data_size = len(traindata)\n",
    "\n",
    "np.random.seed(516)\n",
    "shuffled_indices = np.random.permutation(data_size)\n",
    "#train_indices = shuffled_indices[:int(data_size * 0.8)]\n",
    "#valid_indices = shuffled_indices[int(data_size * 0.8):]\n",
    "train_indices = shuffled_indices[:8]\n",
    "valid_indices = shuffled_indices[8:10]\n",
    "\n",
    "\n",
    "\n",
    "trainset = torch.utils.data.Subset(traindata, train_indices)\n",
    "validset = torch.utils.data.Subset(traindata, shuffled_indices[len(train_indices):])\n",
    "testdata = CustomDataset(\"test_s\") # (100,)\n",
    "testset = torch.utils.data.Subset(testdata, [0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, shuffle=True, collate_fn=collate_batch)\n",
    "validloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, shuffle=False, collate_fn=collate_batch)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, num_workers=4, pin_memory=True, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0fb46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "#   - ResNet based architecture\n",
    "#   - Please refer AlphaFold or trRosetta paper\n",
    "#      AlphaFold1 paper: https://www.nature.com/articles/s41586-019-1923-7\n",
    "#      trRosetta paper: https://www.pnas.org/doi/10.1073/pnas.1914677117\n",
    "\n",
    "# You may want to google how to implement ResNet in pytorch! Good luck!\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, num_layers):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            layer = self._make_layer(in_channels + i * growth_rate, growth_rate)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels):\n",
    "        layer = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "        return layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = [x]\n",
    "        for layer in self.layers:\n",
    "            new_feature = layer(torch.cat(features, dim=1))\n",
    "            features.append(new_feature)\n",
    "        return torch.cat(features, dim=1)\n",
    "\n",
    "class ContactPredictor(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_blocks, growth_rate=32):\n",
    "        super(ContactPredictor, self).__init__()\n",
    "        self.growth_rate = growth_rate\n",
    "        self.dense_blocks = nn.ModuleList()\n",
    "        self.transition_layers = nn.ModuleList()\n",
    "\n",
    "        # declare d_middle inside DenseNet (!=ResNet)\n",
    "        d_middle = growth_rate * 2\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Conv2d(d_in, d_middle, 1) # project down d_in inputs to gr*2 hidden representation\n",
    "        self.bn1 = nn.BatchNorm2d(d_middle)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # DenseNet\n",
    "        for i, num_layers in enumerate(num_blocks):\n",
    "            block = DenseBlock(d_middle, growth_rate, num_layers)\n",
    "            self.dense_blocks.append(block)\n",
    "            d_middle += num_layers * growth_rate\n",
    "            if i != len(num_blocks) - 1:\n",
    "                transition = self._make_transition_layer(d_middle)\n",
    "                self.transition_layers.append(transition)\n",
    "                d_middle //= 2\n",
    "                \n",
    "        # Mapping to output dimension\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((64, 64))\n",
    "        self.to_out = nn.Conv2d(d_middle // 2, d_out, kernel_size=1)\n",
    "        \n",
    "    def _make_transition_layer(self, in_channels):\n",
    "        transition = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, in_channels // 2, kernel_size=1),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        return transition\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: raw 2D features (Batch, d_raw, Length, Length)\n",
    "        # Output: distogram/contact logits (Batch, n_bin, Length, Length)\n",
    "        x = self.embedding(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        for block, transition in zip(self.dense_blocks, self.transition_layers):\n",
    "            x = block(x)\n",
    "            x = transition(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.to_out(x)\n",
    "        x = x.squeeze(1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0826312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss(1/3): 0.7125874283289022\n",
      "Valid loss(1/3): 0.6563980621278915\n",
      "Train loss(2/3): 0.21494079791773402\n",
      "Valid loss(2/3): 0.5032189166449825\n",
      "Train loss(3/3): 0.3798605948607312\n",
      "Valid loss(3/3): 0.29286239517387\n"
     ]
    }
   ],
   "source": [
    "# 1. Training data -- Prepared\n",
    "# 2. Data loader -- process input features & labels\n",
    "# 3. Deep learning model\n",
    "# 4. Optimizer\n",
    "# 5. Loss function\n",
    "\n",
    "# Get AI model to be trained\n",
    "# You need to change **...** part!\n",
    "# DenseNet-121: [6, 12, 24, 16]\n",
    "# DenseNet-169: [6, 12, 32, 32]\n",
    "# DenseNet-201: [6, 12, 48, 32]\n",
    "# DenseNet-264: [6, 12, 64, 48]\n",
    "model = ContactPredictor(529, 1, [6, 12, 24, 16]).to(device)\n",
    "\n",
    "# define loss function to use (which loss you need to use?)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "for i_epoch in range(NUM_EPOCHS):\n",
    "    avg_loss = 0.0\n",
    "    model.train()\n",
    "    for i_batch, (inputs, labels, masks) in enumerate(trainloader):\n",
    "        inputs = inputs.permute(0,3,1,2).to(device, non_blocking=True) # for Convolution layer in pytorch, input should have (Batch, feature dimension, H, W)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        outputs = model(inputs.float())\n",
    "        loss = (criterion(outputs, labels)*masks).sum() / masks.sum() # use mask to ignore missing regions\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.detach()\n",
    "    avg_loss = avg_loss / len(trainloader)\n",
    "    print (f'Train loss({i_epoch + 1}/{NUM_EPOCHS}): {avg_loss.item()}')\n",
    "    \n",
    "    # Check validation loss\n",
    "    model.eval()\n",
    "    avg_loss = 0.0\n",
    "    with torch.no_grad(): # you don't need to calculate gradient for validation\n",
    "        for i_batch, (inputs, labels, masks) in enumerate(validloader):\n",
    "            inputs = inputs.permute(0,3,1,2).to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "\n",
    "            outputs = model(inputs.float())\n",
    "            loss = (criterion(outputs, labels)*masks).sum() / masks.sum()\n",
    "\n",
    "            avg_loss += loss.detach()\n",
    "    avg_loss = avg_loss / len(validloader)\n",
    "    print (f'Valid loss({i_epoch + 1}/{NUM_EPOCHS}): {avg_loss.item()}')\n",
    "    torch.save({\n",
    "                'epoch': i_epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                }, f'model_ep{i_epoch + 1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "68de85ae-5ab0-4f1d-ac42-fb02c041e877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_ep3.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acf2125a-df6b-48c2-834a-d8f13dc81851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33229279469742323\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "avg_loss = 0.0\n",
    "with torch.no_grad(): # you don't need to calculate gradient for validation\n",
    "    for i_batch, (inputs, labels, masks) in enumerate(testloader):\n",
    "        inputs = inputs.permute(0,3,1,2).to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        outputs = model(inputs.float())\n",
    "        loss = (criterion(outputs, labels)*masks).sum() / masks.sum()\n",
    "        avg_loss += loss.detach()\n",
    "    avg_loss = avg_loss / len(testloader)\n",
    "print(float(avg_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1f3adadbf25c62fc6f766b52b851325b8c1f90b123eb2555aa3b6b591ae4ccb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
